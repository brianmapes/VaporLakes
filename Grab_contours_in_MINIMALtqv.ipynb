{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "structural-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 29 17:44:46 EDT 2023\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-polls",
   "metadata": {},
   "source": [
    "# Grab lake contours bigger than 5 degrees - no time tracking\n",
    "## Make distance and direction field, lake by lake (time by time)\n",
    "#### from all available datafiles, their names end in distance.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-error",
   "metadata": {},
   "source": [
    "------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e5fddf2-ed30-4e2b-9a0a-d52ae75ced24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import geometry\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import fiona             # a read-write library for shapefiles\n",
    "import os         \n",
    "#from descartes.patch import PolygonPatch\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import pyproj\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "#import dask.array as da\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5d4fa78-a41a-4a60-a5fe-e54bd20c042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('/Users/brianmapes/Box/VaporLakes/data/LAKEBYLAKE/MERRA2_2D/MINIMAL/*distance.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd4ef818-3594-40b0-8e3c-74139dc0ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRS warnings are annoying below, I might suppress all just for readability\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b915fec-e721-4896-a825-41a4e62812ad",
   "metadata": {},
   "source": [
    "# A function to return a GeoDataFrame of polygons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c0eb40c-a10d-424e-aaee-09cb7531c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over contour collections (and polygons in each collection)\n",
    "# store in polylist  \n",
    "def gdf_from_contours(lon,lat,tqv,conlevel):\n",
    "    \n",
    "    levels = [conlevel, 9e9]\n",
    "    cs = plt.contourf(lon,lat,tqv,levels)\n",
    "# create lookup table for levels\n",
    "    lvl_lookup = dict(zip(cs.collections, cs.levels))\n",
    "    \n",
    "    zvalues, polylist  = [], []\n",
    "#    i=0\n",
    "    for col in cs.collections:\n",
    "        z=lvl_lookup[col] # the value of this level\n",
    "        for contour_path in col.get_paths():\n",
    "#        print('contour path: ',i); i = i+1\n",
    "        # create the polygon for this level\n",
    "            for ncp,cp in enumerate(contour_path.to_polygons()):\n",
    "#            print('   ncp: ', ncp)\n",
    "                lons = np.array(cp)[:,0]\n",
    "                lats = np.array(cp)[:,1]\n",
    "                new_shape = geometry.Polygon([(i[0], i[1]) for i in zip(lons,lats)])            \n",
    "                if ncp == 0:\n",
    "                    poly = new_shape # first shape\n",
    "                else:\n",
    "                    poly = poly.difference(new_shape) # Remove the holes\n",
    "\n",
    "            polylist.append(poly)\n",
    "            zvalues.append(z)\n",
    "        \n",
    "        gdf = gp.GeoDataFrame(geometry=polylist)\n",
    "        gdf['tqv_values']=zvalues\n",
    "        gdf['perimeter']=gdf.length\n",
    "        gdf['area']=gdf.area\n",
    "        gdf['centroidlat']=gdf.centroid.y\n",
    "        gdf['centroidlon']=gdf.centroid.x\n",
    "        gdf['centriod_is_inside']= gdf.contains(gdf.centroid)\n",
    "        gdf['maxlon']=gdf.bounds.maxx\n",
    "        gdf['minlon']=gdf.bounds.minx\n",
    "        gdf['maxlat']=gdf.bounds.maxy\n",
    "        gdf['minlat']=gdf.bounds.miny\n",
    "\n",
    "        return(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62998ef-fc6e-430e-9025-38adb6316df9",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "# Open each dataset in xarray, and loop over all times in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "702695d7-4283-4ce3-a61d-9a46abdaf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINSIZE = 5 # square degrees, smallest area lake (closed contour at an instant) to process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e940e2-800c-46c1-8c38-5537299b6ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_gdflakes = [] # collect them all in one big index set \n",
    "\n",
    "for ifile in range( len(files) ):  \n",
    "    file = files[ifile] # replace with a loop to do all the MERRA2_MINIMAL data files\n",
    "    print(file)\n",
    "    bigds = xr.open_dataset(file)\n",
    "    \n",
    "    # Make an array of Points that are the gridpoints of MERRA2 in this file\n",
    "    lat2d = bigds.lat.values[:,None]   + bigds.lon.values*0\n",
    "    lon2d = bigds.lat.values[:,None]*0 + bigds.lon.values\n",
    "    points = gp.GeoSeries( [Point(x, y) for x, y in zip(lon2d.ravel(),lat2d.ravel()) ] )\\\n",
    "            .set_crs(epsg = \"4326\", inplace = True)\n",
    "     \n",
    "# TIME LOOP OVER ALL TIMES: SLICE OFF ds DATASET - 2D ONLY\n",
    "    for itime in range(len(bigds.time)): \n",
    "        ds = bigds.isel(time=itime).drop_vars(['distance','dir_from_centroid'])\n",
    "        gdf = gdf_from_contours(ds.lon,ds.lat,ds.tqv, 55.)\n",
    "    \n",
    "        # Screen for \"LAKES\" inbounds and big enough (>5 square degrees) \n",
    "        inbounds = \\\n",
    "            (gdf.minlon > ds.lon.min().values) & (gdf.maxlon < ds.lon.max().values) & \\\n",
    "            (gdf.minlat > ds.lat.min().values) & (gdf.maxlat < ds.lat.max().values)\n",
    "        gdflakes = gdf[ (inbounds == True) & (gdf.area > MINSIZE) ].copy().reindex()\n",
    "    \n",
    "# Now loop over all those lakes, making a distance field for each lake\n",
    "        i=0 # counter for lakes at this time\n",
    "        for lake in gdflakes.itertuples():\n",
    "# Compute distance field from perimiter:\n",
    "            dist = points.distance(lake.geometry.boundary).values.reshape(len(ds.lat),len(ds.lon))\n",
    "            isin = points.within(lake.geometry).values.reshape(len(ds.lat),len(ds.lon))\n",
    "            dist *= (-2)*(isin-0.5)  # make SIGNED distance from boundary, positive is exterior \n",
    "\n",
    "# Direction to centroid of WHOLE LAKE at this time\n",
    "            centlon = lake.centroidlon\n",
    "            centlat = lake.centroidlat \n",
    "# use j in explicit loop over gridpoints, since geodesic takes scalar only. Inelegant but works. \n",
    "            dir_to = []\n",
    "            for j in range(len(lon2d.ravel())): \n",
    "                fwd_az,back_az,d = geodesic.inv(lon2d.ravel()[j], lat2d.ravel()[j], [centlon], [centlat])\n",
    "                dir_to.append(fwd_az+180)\n",
    "                \n",
    "            dist_to = np.array(dist).reshape(len(ds.lat),len(ds.lon))\n",
    "            dir_to = np.array(dir_to).reshape(len(ds.lat),len(ds.lon))\n",
    "\n",
    "            command = \"ds = ds.assign(distance\"+str(i)+\"=(['lat','lon'],dist))\"\n",
    "            exec(command)\n",
    "            command = \"ds = ds.assign(dir_to\"+str(i)+\"=(['lat','lon'],dir_to))\"\n",
    "            exec(command)\n",
    "            i += 1 \n",
    "            \n",
    "            # write geojson of lakes at this time into one file -- no need really \n",
    "            # keeping all_gdflakes so that can be used later \n",
    "            #with open(file[:-2]+str(itime)+'lakes_contours.geojson', 'w') as f:\n",
    "            #    f.write(gdflakes.to_json())\n",
    "            #    f.close()\n",
    "            \n",
    "## SHOULD REALLY PUT THESE IN FOLDERS, BUT EASY IS JUST MAKE A FILENAME FOR NOW TO SEE IF IT WORKS \n",
    "            ds.to_netcdf(file[:-2]+str(itime)+'_distdirs.nc')\n",
    "            ds.close()\n",
    "                     \n",
    "        all_gdflakes.append(gdflakes) # giant list of all lakes at all times in all files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gdflakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0363a3-eeac-4cd1-b732-6455e2230ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
